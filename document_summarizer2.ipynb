{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1xlKF6T0mTkP_DUrzO8iW87HcdT8laO1i",
      "authorship_tag": "ABX9TyOxGSWQ2X1OiekJN837nR+C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaaramada/documentsummrize/blob/main/document_summarizer2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aofzVY33RdUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5edf54e-baf9-44d4-e653-98a4136fad74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "\n",
        "import re\n",
        "import fitz\n",
        "\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/DocumentSummarizer/adib.pdf'\n",
        "all_text_path = '/content/drive/MyDrive/DocumentSummarizer/adib2.txt'\n",
        "output_txt2 = '/content/drive/MyDrive/DocumentSummarizer/adib.txt'\n",
        "adib_summary = '/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt'\n",
        "adib_summary2 = '/content/drive/MyDrive/DocumentSummarizer/adib_summary2.txt'\n",
        "title_txt = '/content/drive/MyDrive/DocumentSummarizer/adib_title.txt'\n",
        "\n",
        "\n",
        "categories = {\n",
        "    'Environment': ['energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction'],\n",
        "    'Social': ['fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor'],\n",
        "    'Governance': ['governance', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "    }\n",
        "\n",
        "def pdf_to_text_All(pdf_path, all_text_path):\n",
        "    # Open the PDF file in read-binary mode\n",
        "    doc = fitz.open(pdf_path)\n",
        "    unicode_chars_pattern = r'[\\u201c\\u201d\\u2019s]'\n",
        "    social_count = 0\n",
        "    text =\"\"\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "            for block in blocks:\n",
        "                if block['type'] == 0:  # text block\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            text += span[\"text\"]\n",
        "                            text += \"\\n\"\n",
        "\n",
        "        text = re.sub(unicode_chars_pattern, '', text)\n",
        "        text = re.sub(r'\\n', ' ', text)  # Remove newlines\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    with open(all_text_path, 'w', encoding='utf-8') as txt_file:\n",
        "        txt_file.write(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Upload the file\n",
        "#uploaded = files.upload()\n",
        "# Get the filename from the uploaded dictionary\n",
        "#pdf_path = next(iter(uploaded.keys()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "\n",
        "def extract_first_pages(pdf_path, title_txt, num_pages=5):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in range(min(num_pages, len(reader.pages))):\n",
        "            text += reader.pages[page].extract_text()\n",
        "\n",
        "    with open(title_txt, 'w', encoding='utf-8') as txt_file:\n",
        "        txt_file.write(text)\n",
        "    print(text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "A1JDzN45oh40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17750a7f-42ce-4ab1-fad3-b83f1dc005e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_content_by_headlines2(title_txt):\n",
        "    doc = fitz.open(title_txt)\n",
        "    pattern = re.compile(r'\\.{2,}\\s*\\d+[A-Z\\s]*')\n",
        "    content_groups = []\n",
        "    current_group = {}\n",
        "    current_group[\"title\"] = \"\"\n",
        "    current_group[\"content\"] = \"\"\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "\n",
        "        for block in blocks:\n",
        "            if block['type'] == 0:  # text block\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line[\"spans\"]:\n",
        "                        text = span[\"text\"].strip()\n",
        "                        # Check if the text is in all capital letters\n",
        "                        if text.isupper() and len(text) > 2 and not pattern.match(text):  # Adjust minimum length as needed\n",
        "                            if current_group[\"content\"]:\n",
        "                                content_groups.append(current_group)\n",
        "                                current_group = {\"title\": \"\", \"content\": \"\"}\n",
        "                            current_group[\"title\"] = text\n",
        "                        elif pattern.match(text):\n",
        "                            current_group[\"content\"] += text + \" \"\n",
        "                        else:\n",
        "                            current_group[\"content\"] += text + \" \"\n",
        "\n",
        "    # Add the final group if it contains content\n",
        "    if current_group[\"content\"] or current_group[\"title\"]:\n",
        "        content_groups.append(current_group)\n",
        "\n",
        "    # Remove the dublication if found\n",
        "    seen_titles = set()\n",
        "    unique_data = []\n",
        "    for current_group[\"title\"], current_group[\"content\"] in content_groups:\n",
        "        if current_group[\"title\"] not in seen_titles:\n",
        "            current_group[\"title\"] = \"last_title\"\n",
        "            current_group[\"content\"] = \"last_content\"\n",
        "            unique_data.append(content_groups)\n",
        "            seen_titles.add(current_group[\"title\"])\n",
        "    print(content_groups)\n",
        "    return content_groups"
      ],
      "metadata": {
        "id": "LU2rw4riwv00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_content_to_titles(text, content_groups, output_txt2, file_summary, file_summary2):\n",
        "    titles = [group[\"title\"] for group in content_groups]  # Create an array with just the titles\n",
        "    title_count = len(titles)  # Count the number of titles\n",
        "\n",
        "    catogry = \"\"\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    concatenated_text = ' '.join(lines)\n",
        "\n",
        "\n",
        "    with open(output_txt2, 'w', encoding='utf-8') as txt_file,\\\n",
        "         open(file_summary, 'w', encoding='utf-8') as txt_file2,\\\n",
        "         open(file_summary2, 'w', encoding='utf-8') as txt_file3:\n",
        "        for i in range(0, len(titles) - 1):\n",
        "                first_headline = titles[i]\n",
        "                if first_headline == \"last_title\": continue\n",
        "                headline_length = len(first_headline)\n",
        "                start_indices = [m.start() for m in re.finditer(re.escape(first_headline), concatenated_text)]\n",
        "                start_after_headline = start_indices[0] + headline_length\n",
        "                concatenated_text = concatenated_text[start_after_headline:]\n",
        "\n",
        "        #txt_file.write(concatenated_text + \"\\n\")\n",
        "        summaries = []\n",
        "        Fsummaries = []\n",
        "        for i in range(0, len(titles) -1):\n",
        "            first_headline = titles[i]\n",
        "            try:\n",
        "                second_headline = titles[i+1]\n",
        "            except IndexError:\n",
        "                second_headline = first_headline\n",
        "            except Exception as e:\n",
        "                second_headline = first_headline\n",
        "\n",
        "\n",
        "            print(\"**************\\n\")\n",
        "            print(first_headline)\n",
        "            print(\"\\n\",second_headline)\n",
        "            #txt_file.write(\"\\n \" + first_headline + \".\"*50 + \"\\n\")\n",
        "\n",
        "            c = 0\n",
        "            x = 0\n",
        "            start_indices = [m.start() for m in re.finditer(re.escape(first_headline), concatenated_text)]\n",
        "            print(start_indices)\n",
        "            print(len(start_indices))\n",
        "            last = len(titles) - 2\n",
        "            if (i == last and second_headline == \"last_title\") or (i == len(titles) and first_headline == \"last_title\"):\n",
        "                in_between = concatenated_text[start_indices[c]:]\n",
        "                if not in_between.strip(): continue\n",
        "                catogry = categorize_text(in_between, categories)\n",
        "                if catogry != \"Not_Found\" or catogry != \"\":\n",
        "                    summary = summarize_text(in_between, True)\n",
        "                    Fsummary = summarize_text(summary, False)\n",
        "                    summaries.append(summary)\n",
        "                    Fsummaries.append(Fsummary)\n",
        "                    print(\"Fsummaries\", Fsummaries)\n",
        "                    txt_file2.write(summary + \"\\n\")\n",
        "                    txt_file3.write(Fsummary + \"\\n\")\n",
        "                    print(\"catogry is:\", catogry)\n",
        "                print(\"content of: \", first_headline)\n",
        "                txt_file.write(in_between + \"\\n\")\n",
        "                #txt_file.write(\"\\n NEW_PRAG\" + \".\"*50 + \"\\n\")\n",
        "                break\n",
        "\n",
        "            if len(start_indices) == 0:\n",
        "                print(\"Zero section for start\")\n",
        "                start = find_partial_matches(concatenated_text, first_headline)\n",
        "                if start != 0:\n",
        "                    start_indices = [start]\n",
        "                    print(\"match start\", start)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            if first_headline == second_headline:\n",
        "                continue\n",
        "            else:\n",
        "                end_indices = [m.start() for m in re.finditer(re.escape(second_headline), concatenated_text)]\n",
        "                print(end_indices)\n",
        "                print(len(end_indices))\n",
        "                if len(end_indices) == 0:\n",
        "                    print(\"Zero section for end\")\n",
        "                    end = find_partial_matches(concatenated_text, second_headline)\n",
        "                    print(end)\n",
        "                    if end != 0:\n",
        "                        end_indices = [end]\n",
        "                        end_indices = check_indicator(start_indices, end_indices)\n",
        "                        in_between = concatenated_text[start_indices[c]:end_indices[x]]\n",
        "                        if not in_between.strip(): continue\n",
        "                        catogry = categorize_text(in_between, categories)\n",
        "                        print(catogry)\n",
        "                        if catogry != \"Not_Found\" or catogry != \"\":\n",
        "                            summary = summarize_text(in_between, True)\n",
        "                            Fsummary = summarize_text(summary, False)\n",
        "                            summaries.append(summary)\n",
        "                            Fsummaries.append(Fsummary)\n",
        "                            print(\"Fsummaries\", Fsummaries)\n",
        "                            txt_file2.write(summary + \"\\n\")\n",
        "                            txt_file3.write(Fsummary + \"\\n\")\n",
        "                            print(\"catogry is:\", catogry)\n",
        "                        print(\"content of: \", first_headline)\n",
        "                        txt_file.write(in_between + \"\\n\")\n",
        "\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                else:\n",
        "                    end_indices = check_indicator(start_indices, end_indices)\n",
        "                    in_between = concatenated_text[start_indices[c]:end_indices[x]]\n",
        "                    if not in_between.strip(): continue\n",
        "                    catogry = categorize_text(in_between, categories)\n",
        "                    if catogry != \"Not_Found\" or catogry != \"\":\n",
        "                        summary = summarize_text(in_between, True)\n",
        "                        Fsummary = summarize_text(summary, False)\n",
        "                        summaries.append(summary)\n",
        "                        Fsummaries.append(Fsummary)\n",
        "                        print(\"Fsummaries\", Fsummaries)\n",
        "                        txt_file2.write(summary + \"\\n\")\n",
        "                        txt_file3.write(Fsummary + \"\\n\")\n",
        "                        print(\"catogry is:\", catogry)\n",
        "                    print(\"content of: \", first_headline)\n",
        "                    txt_file.write(in_between + \"\\n\")\n",
        "\n",
        "\n",
        "    #print(Fsummaries)\n",
        "    return summaries\n",
        "\n",
        "def check_indicator(start_indices, end_indices):\n",
        "    endicator = end_indices\n",
        "    if end_indices[0] < start_indices[0]:\n",
        "        if len(end_indices) > 1:\n",
        "            for l in range(0, len(end_indices)):\n",
        "                if start_indices[0] < end_indices[l]:\n",
        "                    endicator = [end_indices[l]]\n",
        "    return endicator\n",
        "\n",
        "def find_partial_matches(text, headline):\n",
        "    # Split the title into words\n",
        "    title_words = headline.split()\n",
        "    # Convert the text to lower case for case-insensitive comparison\n",
        "    text_lower = text\n",
        "    # Check if any word from the title is present in the text\n",
        "    start_position = 0\n",
        "    for word in title_words:\n",
        "        match = re.search(r'\\b' + re.escape(word) + r'\\b', text_lower, re.IGNORECASE)\n",
        "        if match:\n",
        "            start_position = match.start()\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "    return start_position\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def categorize_text(text, categories):\n",
        "    # Preprocess the text\n",
        "    text = preprocess_text(text)\n",
        "    # Dictionary to keep track of keyword matches for each category\n",
        "    category_matches = {category: 0 for category in categories}\n",
        "    # Check for keyword matches in each category\n",
        "    for category, keywords in categories.items():\n",
        "        for keyword in keywords:\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
        "                category_matches[category] += 1\n",
        "\n",
        "    # Find the category with the highest match count\n",
        "    max_category = max(category_matches, key=category_matches.get)\n",
        "    max_count = category_matches[max_category]\n",
        "\n",
        "    # Return the category with the most matches or None if no matches\n",
        "    if max_count > 0:\n",
        "        return max_category\n",
        "    else:\n",
        "        return \"Not_Found\"\n"
      ],
      "metadata": {
        "id": "a1K_-XXzwUmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
        "!pip install transformers\n",
        "!pip install numpy==1.24.3\n",
        "\n",
        "!pip install numpy scipy scikit-learn\n",
        "\n",
        "from transformers import pipeline\n",
        "import textwrap\n",
        "\n",
        "def summarize_text(text, bolean):\n",
        "  summaries = []\n",
        "  model='facebook/bart-large-cnn'\n",
        "  max_length = 1024\n",
        "  overlap = 100\n",
        "  try:\n",
        "      chunks = textwrap.wrap(text, width=max_length - overlap)\n",
        "      #chunks =  split_line_into_chunks(text)\n",
        "      #print(chunks)\n",
        "\n",
        "      summaries = []\n",
        "      for chunk in chunks:\n",
        "          text_length = len(chunk)\n",
        "          if text_length < 100:\n",
        "            if text_length < 10: continue\n",
        "            summary_max_length = 10\n",
        "            summary_min_length = 5\n",
        "          elif text_length < 1025:\n",
        "            summary_max_length = 10\n",
        "            summary_min_length = 5\n",
        "          else:\n",
        "            summary_max_length = 20\n",
        "            summary_min_length = 5\n",
        "          if bolean and text_length > 100:\n",
        "            summary_max_length = max(1, int(text_length * 0.05))  # Ensure it's at least 1\n",
        "            summary_min_length = min(1, int(text_length * 0.01))\n",
        "\n",
        "          #print(\"summary_max_length\", summary_max_length)\n",
        "          summarizer2 = pipeline('summarization', model=model)\n",
        "          summary = summarizer2(chunk, max_length=summary_max_length, min_length=summary_min_length, do_sample=False)\n",
        "          summaries.append(summary)\n",
        "          #print(\"sumarry: \", summary)\n",
        "\n",
        "      #print(\"end summary loop\")\n",
        "      #print(summaries)\n",
        "      combined_summary = \" \".join([summary[0]['summary_text'] for summary in summaries])\n",
        "      #print(\"combined_summary\", combined_summary)\n",
        "      return combined_summary\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"Error during summarization: {e}\")\n",
        "      return \"no sumarize text\"\n",
        "\n",
        "#sum_text = \"IB financing is increasingly geared towards a sustainable economy. 13 green financing transactions were closed, including AED1.7 billion green financing for three sustainable sewage treatment plants across Saudi Arabia. ADIB strives to create wider positive impacts for the society. This is achieved through its ethical Shari’a principles and responsible, innovative products and services that make banking more affordable and sustainable. Dubai has demonstrated one of the lowest levelized cost of electricity in the world of USD cents 1.6953 per kWh. Project involves the construction of a state-of-the-art 900MW solar PV plant, using bi-facial panels. Shuaa Energy 3 P.S.C. was structured as a c. 27-year soft mini perm financing with both conventional and Islamic tranches. The financing structure featured a set of equity bridge finances provided by local banks and also by DEWA. Jubail-3A IWP will utilize reverse osmosis (RO) technology to yield a capacity of 600,000 cubic meters a day. The project will benefit from a 25-year Water Purchase Agreement (WPA) with Saudi Water Partnership Company (SWPA) The plant is located in Jubail, Saudi Arabia Jazlah Water Desalination Company USD 481 million September 2020 Construction of Water Treatment Plant 26 Years Year 6 Istina ljara Hogan Lovells Covington Mandated Lead Arranger. ADIB has invested AED 1.8 billion in sustainable/green Sukuks. 93.6% of financing provided to corporates assets were subject to a combined social or environmental screening. A total value of USD 1.7 billion in sustainable project financing has been identified. ADIB’s Credit Risk Policy will be updated to include an ESG insert.  ADIB is dedicated to growing its reputation as a leading Islamic bank, exercising sound governance practices that focus on Shari’a compliance. Measuring portfolio carbon emissions is essential to be able to reduce them. Shari’a financing has steadily increased, amounting to AED 113 billion in 2022, a 22% increase on 2021. All employees are trained in these criteria, with a total of 104 hours delivered in 2022 (an increase of 4% on 2021) ADIB portfolio held around AED 1.8 billion of Green Sukuk in 2022. Proceeds from both were invested in projects such as renewable energy, energy efficiency, sustainable water management and green buildings. In 2022 the bank accelerated efforts against its three-year Digital Transformation Strategy. Examples of innovation initiatives in 2022 included expanding Interactive Teller Machines. Facial recognition artificial intelligence (AI) was used in 40% of account openings in 2022. The proportion of digitally active customers increased to 76%, with one new digital product and a 31.1% rise in online or mobile transactions. ADIB launched ACE, the region’s first digital command centre. Digital Excellence remains at the heart of ADIB’s 2025 strategy. The bank strives to become a digital-first financial institution. For customers, this means an even more seamless experience.\"\n",
        "#Sum2_txt=  \"ADIB strives to create wider positive impacts for the society. 13 green financing transactions were closed, including AED1.7 billion green financing for three sustainable sewage treatment plants across Saudi Arabia. ADIB has invested AED 1.8 billion in sustainable/green Sukuks. 93.6% of financing provided to corporates assets were subject to a combined social or environmental screening. A total value of USD 1.7 billion inustainable project financing has been identified. ADIB portfolio held around AED 1.8 billion of Green Sukuk in 2022. Shari’a financing has steadily increased, amounting to AED 113 billion in 2022, a 22% increase on 2021. ADIB aims to become a digital-first financial institution. For customers, this means an even more seamless experience.\"\n",
        "#summarize_text(sum_text)"
      ],
      "metadata": {
        "id": "ZxC07n1yx6iB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbe2aae-cdd9-443b-9ba5-d1d5c51ebd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.30.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torchaudio==2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "!pip install matplotlib\n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def word_cloud(sum_text):\n",
        "    # Generate a word cloud\n",
        "    file_path = \"word_cloud.png\"\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='black', max_words=30, stopwords=STOPWORDS).generate(sum_text)\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    #plt.show()\n",
        "\n",
        "    # Save to a BytesIO object\n",
        "    wordcloud.to_image().save(file_path, format='png')\n",
        "\n",
        "\n",
        "    plt.savefig(file_path, format='png')\n",
        "    plt.close()\n",
        "\n",
        "    return file_path\n",
        "\n"
      ],
      "metadata": {
        "id": "RFRjbwnyBP2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b1a38f-6024-4787-a45b-5c99bde8eafe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.24.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_as_spilit(file):\n",
        "  with open(file, \"r\") as fp:\n",
        "    str = fp.read().splitlines()\n",
        "\n",
        "  return str\n",
        "\n",
        "def read_as_text(file):\n",
        "  with open(file, \"r\") as fp:\n",
        "    str = fp.read()\n",
        "  return str\n",
        "\n",
        "#read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "\n"
      ],
      "metadata": {
        "id": "_5KkQQ0PVWnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def re_summarizing(sum_text):\n",
        "  processed_text = process_text(sum_text)\n",
        "  for line_chunks in processed_text:\n",
        "    print(line_chunks)\n",
        "    sumrized = sum_google_peg(documents2)\n",
        "    print(sumrized)\n",
        "\n",
        "\n",
        "def split_text_by_newlines(text):\n",
        "  return text.splitlines()\n",
        "\n",
        "\n",
        "def split_line_into_chunks(line):\n",
        "  sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', line)\n",
        "  chunks = [sentence.strip() for sentence in sentences if sentence]\n",
        "  #chunks = [line[i:i+width] for i in range(0, len(line), width)]\n",
        "  return chunks\n",
        "\n",
        "def process_text(text):\n",
        "  lines = split_text_by_newlines(text)\n",
        "  result = [split_line_into_chunks(line) for line in lines]\n",
        "  return result\n",
        "\n",
        "#text = read_as_text('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#re_summarizing(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EVN5EflVDxWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mglearn\n",
        "import io\n",
        "import sys\n",
        "import mglearn\n",
        "\n",
        "def classification(DATA):\n",
        "  # Define the Vectorizer\n",
        "  VECT: CountVectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "  # Transform the data to create the document-term matrix\n",
        "  FIN = VECT.fit_transform(DATA)\n",
        "  feature_names = VECT.get_feature_names_out()\n",
        "  # Convert to DataFrame for inspection\n",
        "  pd.DataFrame(FIN.toarray(), columns=VECT.get_feature_names_out()).head(1)\n",
        "  # Fit the LDA model\n",
        "  LDA: LatentDirichletAllocation = LatentDirichletAllocation(n_components=3, random_state=42)\n",
        "  LDA_DTF = LDA.fit_transform(FIN)\n",
        "\n",
        "  topics = []\n",
        "  for idx, topic in enumerate(LDA.components_):\n",
        "      top_features_ind = topic.argsort()[:-10 - 1:-1]\n",
        "      top_features = [feature_names[i] for i in top_features_ind]\n",
        "      topics.append(top_features)\n",
        "\n",
        "  print(topics)\n",
        "  predefined_categories = {\n",
        "    'Environment': ['energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction'],\n",
        "    'Social': ['fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor'],\n",
        "    'Governance': ['governance', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "    }\n",
        "  topic_category_mapping = {}\n",
        "  category_scores = {}\n",
        "  for category, keywords in predefined_categories.items():\n",
        "    # Calculate overlap between topic words and category keywords\n",
        "    score = len(set(topic) & set(keywords))\n",
        "    category_scores[category] = score\n",
        "\n",
        "  # Choose the category with the highest overlap score\n",
        "  mapped_category = max(category_scores, key=category_scores.get)\n",
        "\n",
        "  # Store the topic and its mapped category in the dictionary\n",
        "  topic_category_mapping[f\"Topic {idx+1}\"] = {\n",
        "      \"top_words\": topic,\n",
        "      \"category\": mapped_category\n",
        "  }\n",
        "\n",
        "  return topic_category_mapping\n",
        "\n",
        "def classification2(DATA):\n",
        "    # Define the Vectorizer\n",
        "  VECT: CountVectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "  # Transform the data to create the document-term matrix\n",
        "  FIN = VECT.fit_transform(DATA)\n",
        "  feature_names = VECT.get_feature_names_out()\n",
        "  # Convert to DataFrame for inspection\n",
        "  pd.DataFrame(FIN.toarray(), columns=VECT.get_feature_names_out()).head(1)\n",
        "  # Fit the LDA model\n",
        "\n",
        "  LDA: LatentDirichletAllocation = LatentDirichletAllocation(n_components=3)\n",
        "  LDA_DTF = LDA.fit_transform(FIN)\n",
        "\n",
        "  sorting = np.argsort(LDA.components_)[:, ::-1]\n",
        "  features = np.array(VECT.get_feature_names_out())\n",
        "  array = np.full((1, sorting.shape[1]), 1)\n",
        "  array = np.concatenate((array, sorting), axis=0)\n",
        "\n",
        "  output = io.StringIO()\n",
        "  # Redirect stdout to capture print output\n",
        "  original_stdout = sys.stdout\n",
        "  sys.stdout = output\n",
        "  try:\n",
        "    # Call print_topics and capture the output\n",
        "    mglearn.tools.print_topics(\n",
        "        topics=range(1, 4),\n",
        "        feature_names=features,\n",
        "        sorting=array,\n",
        "        topics_per_chunk=5,\n",
        "        n_words=10\n",
        "    )\n",
        "  finally:\n",
        "      # Restore the original stdout\n",
        "      sys.stdout = original_stdout\n",
        "\n",
        "  # Retrieve the captured text\n",
        "  captured_text = output.getvalue()\n",
        "  print(captured_text)\n",
        "  output.close()\n",
        "\n",
        "  return captured_text\n",
        "#DATA = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#topicss = classification(DATA)\n",
        "\n",
        "#print(topicss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fTGfq16aCrpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5ca4b6-74df-4d10-baa8-61f046392a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mglearn in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mglearn) (1.24.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mglearn) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mglearn) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mglearn) (2.1.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from mglearn) (9.4.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from mglearn) (0.12.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from mglearn) (2.34.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from mglearn) (1.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mglearn) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mglearn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mglearn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mglearn) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mglearn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mglearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mglearn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mglearn) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mglearn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mglearn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mglearn) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install nltk (if needed)\n",
        "!pip install nltk\n",
        "import nltk\n",
        "# Download the stopwords corpus\n",
        "nltk.download('stopwords')\n",
        "# Import stopwords and test\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    return text.lower().strip()\n",
        "\n",
        "def normalize_terms(terms):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  normalized = []\n",
        "  for term in terms:\n",
        "      # Convert to lowercase\n",
        "      term = term.lower()\n",
        "      # Remove punctuation\n",
        "      term = term.translate(str.maketrans('', '', string.punctuation))\n",
        "      # Remove stop words\n",
        "      if term not in stop_words:\n",
        "          # Stemming\n",
        "          term = stemmer.stem(term)\n",
        "          normalized.append(term)\n",
        "  return normalized\n",
        "\n",
        "\n",
        "def get_top_words_for_topics(model, feature_names, n_words=10):\n",
        "    topics = model.components_\n",
        "    top_words = {}\n",
        "    for i, topic in enumerate(topics):\n",
        "        top_indices = topic.argsort()[-n_words:][::-1]\n",
        "        top_words[f\"Topic {i+1}\"] = [feature_names[index] for index in top_indices]\n",
        "    return top_words\n",
        "\n",
        "\n",
        "\n",
        "def get_top_word_and_normalize(documents):\n",
        "  vectorizer = CountVectorizer()\n",
        "  X = vectorizer.fit_transform(documents)\n",
        "\n",
        "  lda = LatentDirichletAllocation(n_components=3, random_state=0)\n",
        "  lda.fit(X)\n",
        "\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "  top_words_per_topic = get_top_words_for_topics(lda, feature_names)\n",
        "  top_words_per_topic = {topic: normalize_terms(words) for topic, words in top_words_per_topic.items()}\n",
        "\n",
        "  return top_words_per_topic\n",
        "\n",
        "def get_top_topic_and_normalize(documents):\n",
        "  top_words_per_topic = get_top_word_and_normalize(documents)\n",
        "  topics = set()\n",
        "  for x in top_words_per_topic:\n",
        "    for y in top_words_per_topic[x]:\n",
        "      topics.add(y)\n",
        "  top_topic = topics\n",
        "  top_topic_wo  = top_words_per_topic\n",
        "  return topics\n",
        "#documents = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#print(get_top_word_and_normalize(documents))\n",
        "#print(get_top_topic_and_normalize(documents))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9nQE0eI4CThd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c85692d-8be1-4aac-8e82-263c464d635b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "\n",
        "def sum_google_peg(documents):\n",
        "  topics = get_top_topic_and_normalize(documents)\n",
        "  model_name = \"google/pegasus-xsum\"\n",
        "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "  model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "  REPORT = \"ESG Report\"\n",
        "  ORGANIZATION = \"First Abudhabi Bank\"\n",
        "  tokens = tokenizer(\" \".join(topics) + f\" Report / Study Named {REPORT} By {ORGANIZATION}\", return_tensors=\"pt\", truncation=True)\n",
        "  summary_ids = model.generate(tokens[\"input_ids\"], max_length=1500, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "  return summary\n",
        "\n",
        "#documents = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "#sum_google_peg(documents)\n",
        "\n"
      ],
      "metadata": {
        "id": "c-3n-Vs7SONl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2326ace-d0cc-4f23-c369-9977e5994c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.30.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (18.1.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download 'punkt' model\n",
        "nltk.download('punkt')\n",
        "\n",
        "documents = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "documents2 = read_as_text('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "\n",
        "categories = {\n",
        "    'Environment': ['energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction'],\n",
        "    'Social': ['fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor'],\n",
        "    'Governance': ['governance', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "    }\n",
        "def Bar_Chart_of_Keyword_Frequency(sum_text, keywords_main, file_name):\n",
        "    file_path = file_name+'.png'\n",
        "    if keywords_main == 'Environment' :\n",
        "        keywords = ['energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction']\n",
        "    elif keywords_main == 'Social' :\n",
        "        keywords = ['fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor']\n",
        "    elif keywords_main == 'Governance' :\n",
        "        keywords = ['governance', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "    else:\n",
        "        keywords = []\n",
        "    # Tokenize the summary text and remove stopwords\n",
        "    nltk.download('stopwords')\n",
        "    tokens = nltk.word_tokenize(sum_text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation and numbers\n",
        "    filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_freq = Counter(filtered_tokens)\n",
        "    # Plot the frequency of the top 10 words\n",
        "    #most_common_words = word_freq.most_common(10)\n",
        "    #words, counts = zip(*most_common_words)\n",
        "\n",
        "    filtered_word_freq = {word: word_freq[word] for word in keywords}\n",
        "\n",
        "    words = list(filtered_word_freq.keys())\n",
        "    frequencies = list(filtered_word_freq.values())\n",
        "\n",
        "    #if you need most common word without frequency\n",
        "    #most_common_words_only = [word for word, freq in most_common_words]\n",
        "    #print(most_common_words_only)\n",
        "\n",
        "    plt.figure(figsize=(4, 2))\n",
        "    plt.barh(words, frequencies, color='skyblue')\n",
        "\n",
        "    #plt.bar(words, counts, color='skyblue')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.title(keywords_main)\n",
        "    plt.subplots_adjust(left=0.3, right=0.7)\n",
        "    plt.tight_layout()\n",
        "    #plt.show()\n",
        "    # Save to a BytesIO object\n",
        "    plt.savefig(file_path, format='png')\n",
        "    plt.close()\n",
        "\n",
        "    return file_path\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def categorize_text(text, categories):\n",
        "    # Preprocess the text\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    # Dictionary to keep track of keyword matches for each category\n",
        "    category_matches = {category: 0 for category in categories}\n",
        "    # Check for keyword matches in each category\n",
        "    for category, keywords in categories.items():\n",
        "        for keyword in keywords:\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
        "                category_matches[category] += 1\n",
        "\n",
        "    # Find the category with the highest match count\n",
        "    max_category = max(category_matches, key=category_matches.get)\n",
        "    max_count = category_matches[max_category]\n",
        "\n",
        "    #print(\"max_category\", max_category)\n",
        "    #print(\"max_count\", max_count)\n",
        "    # Return the category with the most matches or None if no matches\n",
        "    if max_count > 0:\n",
        "        return max_category\n",
        "    else:\n",
        "        return \"Not_Found\"\n",
        "def read_as_text(file):\n",
        "  with open(file, \"r\") as fp:\n",
        "    str = fp.read()\n",
        "  return str\n",
        "\n",
        "#keywords = categorize_text(documents2, categories)\n",
        "#Bar_Chart_of_Keyword_Frequency(documents2,keywords)"
      ],
      "metadata": {
        "id": "ccQd_R3kEaDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "b2e09ff3-fdc8-4f05-bd54-d71111485c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'read_as_spilit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-36bf262db306>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_as_spilit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdocuments2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_as_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'read_as_spilit' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def highlight_text(sum_text, keywords_main):\n",
        "  lines = split_text_by_newlines(sum_text)\n",
        "  final_sum_text = \"\"\n",
        "  for line in lines:\n",
        "    sum_text = line.lower()\n",
        "    if keywords_main == 'Environment' :\n",
        "        keywords = ['energy usage and efficiency', 'climate change strategy', 'waste reduction', 'biodiversity loss', 'greenhouse gas emissions', 'carbon food print reduction']\n",
        "    elif keywords_main == 'Social' :\n",
        "        keywords = ['fair pay and living wages', 'equal employment opportunity', 'employee benefits', 'workplace health and safety', 'community engagement', 'responsible supply chain partnerships', 'adhering to labor']\n",
        "    elif keywords_main == 'Governance' :\n",
        "        keywords = ['governance', 'corporate governance', 'risk management', 'compliance', 'ethical business practices', 'avoiding conflicts of interest', 'accounting integrity and transparency']\n",
        "    else:\n",
        "        keywords = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        sum_text = re.sub(rf'({keyword})', r'<b><font color=\"red\">\\1</font></b>', sum_text, flags=re.IGNORECASE)\n",
        "    final_sum_text += sum_text +\"<br/>\"\n",
        "  print(final_sum_text)\n",
        "  return final_sum_text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ltVeAIllFnx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(text):\n",
        "  word_to_remove = \"no sumarize text\"\n",
        "  # Remove the word\n",
        "  cleaned_text = text.replace(word_to_remove, \"\")\n",
        "  return cleaned_text\n",
        "\n",
        "headlines_file_extract = extract_first_pages(pdf_path, title_txt, 5)\n",
        "headlines = split_content_by_headlines2(title_txt)\n",
        "\n",
        "All_document = pdf_to_text_All(pdf_path, output_txt2)\n",
        "summaries = append_content_to_titles(All_document, headlines, output_txt2, adib_summary, adib_summary2)\n"
      ],
      "metadata": {
        "id": "Hfp8JWfG5jS-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "ac461ca4-4c38-47f2-ce26-3f5f30cd95c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_first_pages' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8d7afd42059e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mheadlines_file_extract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_first_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mheadlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_content_by_headlines2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_first_pages' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For PDF generation\n",
        "!pip install reportlab matplotlib wordcloud pillow numpy\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Image\n",
        "from reportlab.platypus import Spacer\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def create_pdf(sum_text, documents, documents2, sub_documents, sub_documents2, categories):\n",
        "    pdf_file = \"report.pdf\"\n",
        "    #bar_chart_file = 'bar_chart.png'\n",
        "    doc = SimpleDocTemplate(pdf_file, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    normal_style = styles[\"Normal\"]\n",
        "    title_style = styles[\"Heading2\"]  # Use a different style for section titles\n",
        "    subtitle_style = styles[\"Heading3\"]  # Use a smaller heading style for subtitles\n",
        "    elements = []\n",
        "    keywords = categorize_text(documents, categories)\n",
        "    # Title and Date\n",
        "    title = \"ESG Summary Report\"\n",
        "    date_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Add title\n",
        "    elements.append(Paragraph(title, styles[\"Title\"]))\n",
        "    elements.append(Paragraph(f\"Date: {date_str}\", normal_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the title\n",
        "\n",
        "    # Add the title for the highlighted text section\n",
        "    elements.append(Paragraph(\"Abstract_text\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "\n",
        "    # Combine highlighted text into one string\n",
        "    combined_highlighted_text = highlight_text(sub_documents, keywords)\n",
        "    introduction = sum_google_peg(documents2)\n",
        "\n",
        "    introduction_paragraph = f'{introduction}'\n",
        "    elements.append(Paragraph(introduction_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Format the combined text to include bold and red styling\n",
        "    highlighted_paragraph = f'{combined_highlighted_text}'\n",
        "    elements.append(Paragraph(highlighted_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    top_word = get_top_word_and_normalize(documents2)\n",
        "    top_topic = get_top_topic_and_normalize(documents2)\n",
        "    elements.append(Paragraph(\"Top Topic\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    top_topic_paragraph = f'{top_topic}'\n",
        "    elements.append(Paragraph(top_topic_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"Top Word Per Topic\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    top_word_paragraph = f'{top_word}'\n",
        "    elements.append(Paragraph(top_word_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"Classification Topic on Catogry Map\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    topicss = classification(documents2)\n",
        "    topicss_paragraph = f'{topicss}'\n",
        "    elements.append(Paragraph(topicss_paragraph, normal_style))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"Classification Topic on General\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "    topicssg = classification2(documents2)\n",
        "    for line in topicssg.split('\\n'):\n",
        "      if line.strip():  # Only add non-empty lines\n",
        "          paragraph = Paragraph(line, normal_style)\n",
        "          elements.append(paragraph)\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    elements.append(Paragraph(\"Visulaization\", title_style))\n",
        "    elements.append(Spacer(1, 12))  # Add space after the subtitle\n",
        "\n",
        "    bar_chart_file1 = Bar_Chart_of_Keyword_Frequency(documents, 'Environment', 'Environment_bar_chart')\n",
        "    print(\"output:\",bar_chart_file1)\n",
        "    # Add an Image\n",
        "    if os.path.exists(bar_chart_file1):\n",
        "        img = Image(bar_chart_file1, width=500, height=300)\n",
        "        elements.append(img)\n",
        "        elements.append(Spacer(1, 12))\n",
        "    else:\n",
        "        error_text = Paragraph(\"Error: Image not found.\", normal_style)\n",
        "        elements.append(error_text)\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "    bar_chart_file2 = Bar_Chart_of_Keyword_Frequency(documents, 'Social', 'social_bar_chart')\n",
        "    print(\"output:\",bar_chart_file2)\n",
        "    # Add an Image\n",
        "    if os.path.exists(bar_chart_file2):\n",
        "        img = Image(bar_chart_file2, width=500, height=300)\n",
        "        elements.append(img)\n",
        "        elements.append(Spacer(1, 12))\n",
        "    else:\n",
        "        error_text = Paragraph(\"Error: Image not found.\", normal_style)\n",
        "        elements.append(error_text)\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "    bar_chart_file3 = Bar_Chart_of_Keyword_Frequency(documents, 'Governance', 'Governance_bar_chart')\n",
        "    print(\"output:\",bar_chart_file3)\n",
        "    # Add an Image\n",
        "    if os.path.exists(bar_chart_file3):\n",
        "        img = Image(bar_chart_file3, width=500, height=300)\n",
        "        elements.append(img)\n",
        "        elements.append(Spacer(1, 12))\n",
        "    else:\n",
        "        error_text = Paragraph(\"Error: Image not found.\", normal_style)\n",
        "        elements.append(error_text)\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "    word_cloud_file = word_cloud(documents)\n",
        "    if os.path.exists(word_cloud_file):\n",
        "        img = Image(word_cloud_file, width=500, height=300)\n",
        "        elements.append(img)\n",
        "        elements.append(Spacer(1, 12))\n",
        "    else:\n",
        "        error_text = Paragraph(\"Error: Image not found.\", normal_style)\n",
        "        elements.append(error_text)\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "\n",
        "\n",
        "    # Build the PDF\n",
        "    doc.build(elements)\n",
        "\n",
        "# Generate the final summary\n",
        "#sum_google_peg(documents)\n",
        "documents = read_as_text('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "sub_documents = read_as_text('/content/drive/MyDrive/DocumentSummarizer/adib_summary2.txt')\n",
        "documents2 = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary.txt')\n",
        "sub_documents2 = read_as_spilit('/content/drive/MyDrive/DocumentSummarizer/adib_summary2.txt')\n",
        "#sum_text = summarize_text(documents)\n",
        "sum_text = \"\"\n",
        "\n",
        "create_pdf(sum_text, documents, documents2, sub_documents, sub_documents2, categories)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-xMVQ0T3_Zji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf6cb62-94ff-4919-b3d0-695b80cabe85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "financial performance and economic impact data cover<br/>bank welcomed 5,280 new mall<br/>adib is a sharia-compl<br/>adib viion wa revamped to adib ha initiated the development of<br/>adib is committed to adhere to in 2022, five incident of non adib report zero data ecurity<br/>adib eek to create value<br/>adib aim to drive the h adib inveted aed in 2022, adib launched ace<br/>adib exit to erve a amwali is a digital i<br/>adib committed to creating a l adib will alo develop and al ghaf initiative i an ongoing<br/>in 2022, total community invet women make up 71.5% in 2022, more training wa delivered  adib divere taff include in 2022, 36 grievance were filed 568 premie check. were conducted<br/>aed 357,276,444 report provides an overview of our performance service reviewed that the gri content ilamic bank is the first paword prb is a<br/>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['bank', 'adib', '2022', 'esg', 'community', 'employee', 'principle', 'cutomer', 'reponible', 'year'], ['bank', 'adib', '2022', 'employee', 'esg', 'cutomer', 'year', 'gri', 'number', 'report'], ['topic', 'material', 'financial', 'acro', 'tatement', 'monetary', 'achievement', 'provides', 'overview', 'tranparent']]\n",
            "topic 1       topic 2       topic 3       \n",
            "--------      --------      --------      \n",
            "bank          bank          bank          \n",
            "adib          esg           adib          \n",
            "2022          adib          2022          \n",
            "employee      2022          community     \n",
            "cutomer       year          value         \n",
            "gri           sharia        uae           \n",
            "number        compliance    aed           \n",
            "2021          committee     wa            \n",
            "report        ha            60            \n",
            "year          report        cutomer       \n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: Environment_bar_chart.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: social_bar_chart.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: Governance_bar_chart.png\n"
          ]
        }
      ]
    }
  ]
}